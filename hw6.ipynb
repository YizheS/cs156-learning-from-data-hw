{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from hw6_nlt import LinRegNLT2\n",
    "from hw6_dataload import LFD_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6\n",
    "## Overfitting and Deterministic Noise\n",
    "Given hypothesis set &Hscr; of target function f and &Hscr;' &subset; &Hscr;, using &Hscr;' in general will lead to **increased deterministic noise** since we will have less hypotheses available at our disposal to deal with a higher-order determinstic function (and moreso, deterministic noise can only increase by using less than the available number of hypotheses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with Weight Decay\n",
    "\n",
    "**Given :** two-dim input x = (x1, x2) so that &Xscr; = &reals;<sup>2</sup> with label &Yscr; = {-1,1}. \n",
    "\n",
    "**Want :** Linear Regression with a non-linear transformation for classification given by:\n",
    "\n",
    "&phi;(x1, x2) = (1, x1, x2, x1<sup>2</sup>, x2<sup>2</sup>, x1x2, |x1-x2|, |x1+x2|)\n",
    "\n",
    "\n",
    "Classification error is defined as the fraction of misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression without Weight Decay:\n",
      "E_in: 0.028571, E_out: 0.084000\n",
      "Linear Regression with Weight Decay (k = -3):\n",
      "E_in: 0.028571, E_out: 0.080000\n",
      "Linear Regression with Weight Decay (k = -2):\n",
      "E_in: 0.028571, E_out: 0.084000\n",
      "Linear Regression with Weight Decay (k = -1):\n",
      "E_in: 0.028571, E_out: 0.056000\n",
      "Linear Regression with Weight Decay (k = 0):\n",
      "E_in: 0.000000, E_out: 0.092000\n",
      "Linear Regression with Weight Decay (k = 1):\n",
      "E_in: 0.057143, E_out: 0.124000\n",
      "Linear Regression with Weight Decay (k = 2):\n",
      "E_in: 0.200000, E_out: 0.228000\n",
      "Linear Regression with Weight Decay (k = 3):\n",
      "E_in: 0.371429, E_out: 0.436000\n"
     ]
    }
   ],
   "source": [
    "rwd_train = \"hw6_train.dta\"\n",
    "rwd_test = \"hw6_test.dta\"\n",
    "l_reg = math.pow(10.0, -3) #lambda regularization term\n",
    "\n",
    "# load data from external files and init\n",
    "rwd_data = LFD_Data(rwd_train, rwd_test)\n",
    "rwd_algo = LinRegNLT2(rwd_data.dim, l_reg)\n",
    "\n",
    "def rwd_print_error(algo,data):\n",
    "    ein = algo.calc_error(data.train_X, data.train_Y)\n",
    "    eout = algo.calc_error(data.test_X, data.test_Y)\n",
    "    print(\"E_in: %f, E_out: %f\" % (ein, eout))\n",
    "    \n",
    "\n",
    "#train without regularization\n",
    "rwd_algo.train(rwd_data.train_X, rwd_data.train_Y)\n",
    "print(\"Linear Regression without Weight Decay:\")\n",
    "rwd_print_error(rwd_algo, rwd_data)\n",
    "\n",
    "\n",
    "rwd_k = np.arange(-3, 4)\n",
    "\n",
    "for k in rwd_k:\n",
    "    cur_lam = math.pow(10.0, k)\n",
    "    rwd_algo.set_lambda(cur_lam)\n",
    "    rwd_algo.train_reg(rwd_data.train_X, rwd_data.train_Y)\n",
    "    print(\"Linear Regression with Weight Decay (k = %d):\" % k)\n",
    "    rwd_print_error(rwd_algo, rwd_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
