{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from hw6_dataload import LFD_Data\n",
    "from hw6_nlt import LinRegNLT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 7\n",
    "## Validation\n",
    "**Given :** Linear Regression with nonlinear transformation.\n",
    "\n",
    "\n",
    "&Phi;(x) = (1, x<sub>1</sub>, x<sub>2</sub>, x<sub>1</sub><sup>, 2</sup>, x<sub>2</sub><sup>2</sup>, x<sub>1</sub>x<sub>2</sub>, |x<sub>1</sub> - x<sub>2</sub>|, |x<sub>1</sub> + x<sub>2</sub>|)\n",
    "\n",
    "\n",
    "As this transformation is the same as the one from HW6 (as well as the datasets), I will reuse the classes from the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LinReg with NLT using the first 25 examples for training (last 10 for validation) ===\n",
      "k = 3 :\n",
      "E_valid: 0.300000, E_out: 0.420000\n",
      "k = 4 :\n",
      "E_valid: 0.500000, E_out: 0.416000\n",
      "k = 5 :\n",
      "E_valid: 0.200000, E_out: 0.188000\n",
      "k = 6 :\n",
      "E_valid: 0.000000, E_out: 0.084000\n",
      "k = 7 :\n",
      "E_valid: 0.100000, E_out: 0.072000\n",
      "\n",
      "=== LinReg with NLT using the last 10 examples for training (first 25 for validation) ===\n",
      "k = 3 :\n",
      "E_valid: 0.280000, E_out: 0.396000\n",
      "k = 4 :\n",
      "E_valid: 0.360000, E_out: 0.388000\n",
      "k = 5 :\n",
      "E_valid: 0.200000, E_out: 0.284000\n",
      "k = 6 :\n",
      "E_valid: 0.080000, E_out: 0.192000\n",
      "k = 7 :\n",
      "E_valid: 0.120000, E_out: 0.196000\n"
     ]
    }
   ],
   "source": [
    "rwd_train = \"hw6_train.dta\"\n",
    "rwd_test = \"hw6_test.dta\"\n",
    "l_reg = math.pow(10.0, -3) #actually, doesn't use regularization but just copying over from last hw anyways.\n",
    "\n",
    "# load data from external files and init\n",
    "rwd_data = LFD_Data(rwd_train, rwd_test)\n",
    "rwd_algo = LinRegNLT2(rwd_data.dim, 7, l_reg)\n",
    "\n",
    "#editing from hw6 to make more flexible\n",
    "def rwd_print_error(algo,valid_X, valid_Y, out_X, out_Y):\n",
    "    #ein = algo.calc_error(in_X, in_Y)\n",
    "    e_valid = algo.calc_error(valid_X, valid_Y)\n",
    "    eout = algo.calc_error(out_X, out_Y)\n",
    "    print(\"E_valid: %f, E_out: %f\" % (e_valid, eout))\n",
    "    \n",
    "print(\"=== LinReg with NLT using the first 25 examples for training (last 10 for validation) ===\")\n",
    "my_k = np.arange(3,8) #up through which cols (0-idx to use for training)\n",
    "#train without regularization\n",
    "for k in my_k:\n",
    "    print(\"k = %d :\" % k)\n",
    "    rwd_algo.set_k(k)\n",
    "    rwd_algo.train(rwd_data.train_X[:25,:], rwd_data.train_Y[:25])\n",
    "    rwd_print_error(rwd_algo, rwd_data.train_X[25:,:], rwd_data.train_Y[25:], rwd_data.test_X, rwd_data.test_Y)\n",
    "\n",
    "print(\"\")\n",
    "print(\"=== LinReg with NLT using the last 10 examples for training (first 25 for validation) ===\")\n",
    "my_k = np.arange(3,8) #up through which cols (0-idx to use for training)\n",
    "#train without regularization\n",
    "for k in my_k:\n",
    "    print(\"k = %d :\" % k)\n",
    "    rwd_algo.set_k(k)\n",
    "    rwd_algo.train(rwd_data.train_X[25:,:], rwd_data.train_Y[25:])\n",
    "    rwd_print_error(rwd_algo, rwd_data.train_X[:25,:], rwd_data.train_Y[:25], rwd_data.test_X, rwd_data.test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Bias\n",
    "\n",
    "**Want :** Expected values of e, e1, and e2 given that e1 and e2 are independent, uniformly-distributed random variables over [0,1] and e = min(e1, e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected values: \n",
      "e: 0.333768\n",
      "e1: 0.501344\n",
      "e2: 0.499188\n"
     ]
    }
   ],
   "source": [
    "vb_n = 100000 #number of random numbers to generate for each e1 and e2\n",
    "e1 = np.random.uniform(0,1, vb_n)\n",
    "e2 = np.random.uniform(0,1, vb_n)\n",
    "e = np.minimum(e1, e2)\n",
    "\n",
    "ev_e1 = np.average(e1)\n",
    "ev_e2 = np.average(e2)\n",
    "ev_e = np.average(e)\n",
    "\n",
    "print(\"Expected values: \")\n",
    "print(\"e: %f\" % ev_e)\n",
    "print(\"e1: %f\" % ev_e1)\n",
    "print(\"e2: %f\" % ev_e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "**Given :**\n",
    "- data points (x,y): (-1,0), (&rho;, 1), (1,0) &SuchThat; &rho; &ge; 0\n",
    "- choice between two models: constant {h<sub>0</sub>(x) = b}, linear {h<sub>1</sub>(x) = ax + b}\n",
    "\n",
    "**Want :** value of &rho; that ties squared error measures for both models using leave-one-out cross-validation\n",
    "\n",
    "**Note :** N = 3\n",
    "- E<sub>cv</sub> = (1/N) &sum;(n=1;N){e<sub>n</sub>}\n",
    "- e<sub>n</sub> = (h(x) - y)<sup>2</sup>\n",
    "\n",
    "**Notation :**\n",
    "\n",
    "- Let p1 = (-1,0), p2 = (&rho;, 1), p3 = (1, 0)\n",
    "- Let A = {p1, p2}, B = {p1, p3}, C = {p2, p3}\n",
    "- Let h<sub>0,A</sub>(x) be the hypothesis in model 0 from training on A, etc.\n",
    "- Let E<sub>0,A</sub>be the validation error in model 0 on A, etc.\n",
    "- Let E<sub>0</sub> be the total cross-validation error on model 0, etc.\n",
    "\n",
    "### Constant Model\n",
    "- note that training on two points in the constant model case is essentially averaging the Y coordinate.\n",
    "- h<sub>0,A</sub>(x) = (p1.y+p2.y)/2 = (0+1)/2 = 1/2\n",
    "- E<sub>0,A</sub>(x) = (h(x)-y)<sup>2</sup> = (1/2 - p3.y)<sup>2</sup> = (1/2 - 0)<sup>2</sup> = 1/4\n",
    "- h<sub>0,B</sub>(x) = (p1.y+p3.y)/2 = (0 + 0)/2 = 0\n",
    "- E<sub>0,B</sub> = (h(x)-p2.y)<sup>2</sup> = (0-1)<sup>2</sup> = 1\n",
    "- h<sub>0,C</sub>(x) = (p2.y+p3.y)/2 = (1+0)/2 = 1/2\n",
    "- E<sub>0,C</sub> = (h(x)-p1.y)<sup>2</sup> = (1/2-0)<sup>2</sup> = 1/4\n",
    "- E<sub>0</sub> = (1/3) &times; (1/4 + 1 + 1/4) = 1/2\n",
    "\n",
    "### Linear Model\n",
    "- note that training on two points in the linear model case is finding the line that passes through the two points, with the equation of a line being y = mx + b where b is the y-intercept and m = (y2-y1)/(x2-x1)\n",
    "- h<sub>1,A</sub>(x):\n",
    "    - m = (p2.y-p1.y)/(p2.x-p1.x) = (1 - 0)/(&rho; - (-1)) = 1/(&rho; + 1)\n",
    "    - y = mx + b &rArr; 0 = -1 &times; (1/(&rho; + 1) + b &rArr; b = 1/(&rho; + 1)\n",
    "    - = (x+1)/(&rho; + 1)\n",
    "- E<sub>1,A</sub> = (h(p3.x) - p3.y)<sup>2</sup> = ((1+1)/(&rho; + 1) - 0)<sup>2</sup> = (2/(&rho; + 1))<sup>2</sup> = 4/((&rho; + 1)<sup>2</sup>)\n",
    "- h<sub>1,B</sub>(x):\n",
    "    - m = (p3.y-p1.y)/(p3.x-p1.x) = (0-0)/(1 - (-1)) = 0/2 = 0\n",
    "    - y = mx + b &rArr; p1.y = 0 &times; p1.x + b &rArr; 0 = 0 &times; -1 + b &rArr; b = 0\n",
    "    - = 0\n",
    "- E<sub>1,B</sub> = (h(p2.x) - p2.y)<sup>2</sup> = (0 - 1)<sup>2</sup> = 1\n",
    "- h<sub>1,C</sub>(x):\n",
    "    - m = (p3.y-p2.y)/(p3.x-p2.x) = (0-1)/(1-&rho;) = -1/(1-&rho;) = 1/(&rho;-1)\n",
    "    - p3.y = m &times; p3.x + b &rArr; 0 = 1/(&rho;-1) + b &rArr; b = 1/(1-&rho;)\n",
    "    - = (x-1)/(&rho;-1) \n",
    "- E<sub>1,C</sub> = (h(p1.x)-p1.y)<sup>2</sup> = ((-1 - 1)/(&rho;-1) - 0)<sup>2</sup> = 4/((&rho;-1)<sup>2</sup>)\n",
    "- E<sub>1</sub> = (1/3) &times; ( 1 + (4((&rho; + 1)<sup>2</sup> + (&rho; - 1)<sup>2</sup>))/((&rho; + 1)<sup>2</sup>(&rho; - 1)<sup>2</sup>)))\n",
    "\n",
    "\n",
    "### Equating errors\n",
    "\n",
    "E<sub>0</sub> = E<sub>1</sub>\n",
    "\n",
    "&rArr; 1/2 = (1/3) &times; ( 1 + (4((&rho; + 1)<sup>2</sup> + (&rho; - 1)<sup>2</sup>))/((&rho; + 1)<sup>2</sup>(&rho; - 1)<sup>2</sup>)))\n",
    "\n",
    "&rArr; 3/2 = 1 + 4((&rho; + 1)<sup>2</sup> + (&rho; - 1)<sup>2</sup>))/((&rho; + 1)<sup>2</sup>(&rho; - 1)<sup>2</sup>))\n",
    "\n",
    "&rArr; 1/2 = 4((&rho; + 1)<sup>2</sup> + (&rho; - 1)<sup>2</sup>))/((&rho; + 1)<sup>2</sup>(&rho; - 1)<sup>2</sup>))\n",
    "\n",
    "&rArr; 1/8 = ((&rho; + 1)<sup>2</sup> + (&rho; - 1)<sup>2</sup>))/(((&rho; + 1)(&rho; - 1))<sup>2</sup>)\n",
    "\n",
    "&rArr;  1/8 = (2&rho;<sup>2</sup> + 2)/((&rho;<sup>2</sup> - 1)<sup>2</sup>)\n",
    "\n",
    "&rArr;  1/8 = 2(&rho;<sup>2</sup> + 1)/(&rho;<sup>4</sup> - 2&rho;<sup>2</sup> + 1)\n",
    "\n",
    "&rArr;  1/16 = (&rho;<sup>2</sup> + 1)/(&rho;<sup>4</sup> - 2&rho;<sup>2</sup> + 1)\n",
    "\n",
    "&rArr;  16(&rho;<sup>2</sup> + 1) = &rho;<sup>4</sup> - 2&rho;<sup>2</sup> + 1\n",
    "\n",
    "&rArr;  16&rho;<sup>2</sup> + 16 = &rho;<sup>4</sup> - 2&rho;<sup>2</sup> + 1\n",
    "\n",
    "&rArr;  &rho;<sup>4</sup> - 18&rho;<sup>2</sup> - 15 = 0\n",
    "\n",
    "(using the quadratic formula...)\n",
    "\n",
    "&rArr; x<sup>2</sup> = (-b  &plusmn; sqrt(b<sup>2</sup> - 4ac))/2a = (18 &plusmn; sqrt(324 + 60))/2 =  9 &plusmn; 4 &times; sqrt(6)\n",
    "\n",
    "Since we're interested in real solutions and 4 &times; sqrt(6) &ge; 9:\n",
    "\n",
    "**x = sqrt(9 + 4 &times; sqrt(6))**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA vs SVM\n",
    "\n",
    "**Given :**\n",
    "- target function f as line defined on [-1,1] x [-1,1] where anything above the line maps to 1 and -1 otherwise\n",
    "- we wish to compare the performance of PLA vs SVM\n",
    "- I've coded the PLA class in pla.py, and have coded the SVM class in svm.py using the cvxopt library for quadratic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For a training set with N = 10 with 1000 runs ===\n",
      "E_out avg for PLA: 0.089959\n",
      "E_out avg for SVM: 0.079959\n",
      "SVM improves on performance over PLA 68.800000% of the time\n",
      "Avg number of support vectors: 2.168000\n",
      "=== For a training set with N = 100 with 100 runs ===\n",
      "E_out avg for PLA: 0.012510\n",
      "E_out avg for SVM: 0.008450\n",
      "SVM improves on performance over PLA 79.000000% of the time\n",
      "Avg number of support vectors: 2.580000\n"
     ]
    }
   ],
   "source": [
    "from pla import PLA\n",
    "from svm import SVM\n",
    "from line_test import LineTest\n",
    "\n",
    "num_exp = [1000,100]\n",
    "n_vals = [10,100]\n",
    "n_test = 1000 #testing sample size\n",
    "for idx,n in enumerate(n_vals):\n",
    "    eout_pla = np.array([])\n",
    "    eout_svm = np.array([])\n",
    "    sv_svm = np.array([])\n",
    "    cur_exp = num_exp[idx]\n",
    "    my_pla = PLA(0) #threshold for number wrong since linearly separable\n",
    "    my_svm = SVM()\n",
    "    for my_exp in range(cur_exp):\n",
    "        my_test = LineTest(n,n_test)\n",
    "        my_pla.train(my_test.train_set,my_test.train_labels)\n",
    "        my_svm.train(my_test.train_set, my_test.train_labels)\n",
    "        pla_err = my_pla.calc_error(my_test.test_set, my_test.test_labels)\n",
    "        svm_err = my_svm.calc_error(my_test.test_set, my_test.test_labels)\n",
    "        n_sv = my_svm.num_alphas\n",
    "        cur_adv = svm_err < pla_err\n",
    "        eout_pla = np.concatenate((eout_pla, [pla_err]))\n",
    "        eout_svm = np.concatenate((eout_svm, [svm_err]))\n",
    "        sv_svm = np.concatenate((sv_svm, [n_sv]))\n",
    "    eavg_pla = np.average(eout_pla)\n",
    "    eavg_svm = np.average(eout_svm)\n",
    "    svm_adv = 100.0*np.average(np.less_equal(eout_svm, eout_pla))\n",
    "    sv_avg = np.average(sv_svm)\n",
    "    print(\"=== For a training set with N = %d with %d runs ===\" % (n, cur_exp))\n",
    "    print(\"E_out avg for PLA: %f\" % eavg_pla)\n",
    "    print(\"E_out avg for SVM: %f\" % eavg_svm)\n",
    "    print(\"SVM improves on performance over PLA %f%% of the time\" % svm_adv)\n",
    "    print(\"Avg number of support vectors: %f\" % sv_avg)\n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
