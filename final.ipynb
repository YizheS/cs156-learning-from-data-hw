{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hw8_dataload import LFD_Data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL\n",
    "\n",
    "## Nonlinear Transforms\n",
    "\n",
    "A polynomial transform of order Q = 10 applied to &Xscr; of dimension *d* = 2 results in a &Zscr; space of what dimensionality?\n",
    "\n",
    "Note that this number is a triangular number. z<sub>n</sub> be the dimension of the order n transform and let's write down a few examples and a and b be the coordinates in the &Xscr; space. Also, let T<sub>n</sub> be the nth triangular number (with T<sub>0</sub> = 0) and P<sub>n</sub> be the set of terms resulting from the polynomial transform of order n.\n",
    "\n",
    "\n",
    "P<sub>2</sub> &rArr; {1, a, b, a<sup>2</sup>b<sup>0</sup>, ab, a<sup>0</sup>b<sup>2</sup>} &rArr; z<sub>2</sub> = 6 = T<sub>3</sub>\n",
    "\n",
    "P<sub>3</sub> &rArr; P<sub>2</sub> &Union; {a<sup>3</sup>b<sup>0</sup>, a<sup>2</sup>b<sup>1</sup>, a<sup>1</sup>b<sup>2</sup>, a<sup>0</sup>b<sup>3</sup>} &rArr; z<sub>2</sub> + 4 = 10 = T<sub>4</sub>\n",
    "\n",
    "Thus, z<sub>n</sub> = T<sub>n+1</sub> and T<sub>11</sub> = z<sub>10</sub> = 55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "Give average hypothesis g-bar resulting from training the same model &Hscr; on different data sets &Dscr; to get g<sup>(&Dscr;)</sup> &isin; &Hscr; and taking the expected value of g<sup>(&Dscr;)</sup> w.r.t. &Dscr;. We want to consider a model &Hscr; that could result in g-bar &notin; &Hscr;.\n",
    "\n",
    "As the logistic regression function &theta;(s) = e<sup>s</sup>/(1+e<sup>s</sup>) is non-linear, we can expect the averaging done by taking the expected value not to be in &Hscr;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "We **cannot** always determine if there is overfitting by comparing the values of (E<sub>out</sub> - E<sub>in</sub>). One major issue is that we have **no** idea of what E<sub>in</sub> is doing. (E<sub>out</sub> - E<sub>in</sub>) = 0.9 - 0.5 = 0.5 -0.1 = 0.4. In the first case, it's arguable that our hypothesis doesn't really match the target function all that well, it's just bad yet our difference vaue is the same as the second case, where our hypothesis is doing relatively well and more of a candidate of overfitting.\n",
    "\n",
    "Deterministic noise is dependent on the hypothesis set since it's the details of the target function f that it cannot possibly capture. Stochastic noise however is not dependent on the hypthesis set, it's just regular old noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Given :** The regularized weight w<sub>reg</sub> solves:\n",
    "\n",
    "minimize (1/N)&sum;(n=1;N){w<sup>T</sup>x<sub>n</sub>-y<sub>n</sub>)<sup>2</sup> subject to w<sup>T</sup>&Gamma;<sup>T</sup>&Gamma;w &le; C\n",
    "\n",
    "where &Gamma; is a  matrix. \n",
    "\n",
    "**Want :** w<sup>T</sup><sub>lin</sub>&Gamma;<sup>T</sup>&Gamma;w<sub>lin</sub> &rArr; w<sub>reg</sub> = ?\n",
    "\n",
    "Since we wish to find the w (w<sub>reg</sub>) that minimizes the given expression and w<sub>lin</sub> already satisfies the given constrants, w<sub>reg</sub> = w<sub>lin</sub>.\n",
    "\n",
    "Furthermore, minimizing the given function results with the given constraints results in minimizing the in-sample error function with an additional slack term, which can be considered an equation for **augmented error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "Using the dataset of HW8 (US Postal Service Zip Code data set) and implementing the same classifiers as before (one-vs-one with one number being +1, the other being -1, rest ignored, and one-vs-all with one number being +1, the rest -1), we wish to use regularized linear regression this time, **minimizing** the following expression:\n",
    "\n",
    "\n",
    "(1/N) &sum;(n=1;N){w<sup>T</sup>z<sub>n</sub>-y<sub>n</sub>)<sup>2</sup> + (&lambda;/N)w<sup>T</sup>w\n",
    "\n",
    "On x-vs-all classifiers, we let &lambda; = 1 and both omit (z = x = (1,x<sub>1</sub>,x<sub>2</sub>)) and apply a feature transform (z = (1,x<sub>1</sub>,x<sub>2</sub>,x<sub>1</sub>x<sub>2</sub>,x<sub>1</sub><sup>2</sup>,x<sub>2</sub><sup>2</sup>))\n",
    "\n",
    "Then we want to use a 1-vs-5 classifier with the feature tranform and compare the performance with &lambda; = 0.01 and &lambda; = 1.\n",
    "\n",
    "\n",
    "**Note** that minimizing the given function above is equivalent to solving for w<sub>reg</sub> = (Z<sup>T</sup>Z + &lambda;I)<sup>-1</sup>Z<sup>T</sup>y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
