{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hw8_dataload import LFD_Data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL\n",
    "\n",
    "## Nonlinear Transforms\n",
    "\n",
    "A polynomial transform of order Q = 10 applied to &Xscr; of dimension *d* = 2 results in a &Zscr; space of what dimensionality?\n",
    "\n",
    "Note that this number is a triangular number. z<sub>n</sub> be the dimension of the order n transform and let's write down a few examples and a and b be the coordinates in the &Xscr; space. Also, let T<sub>n</sub> be the nth triangular number (with T<sub>0</sub> = 0) and P<sub>n</sub> be the set of terms resulting from the polynomial transform of order n.\n",
    "\n",
    "\n",
    "P<sub>2</sub> &rArr; {1, a, b, a<sup>2</sup>b<sup>0</sup>, ab, a<sup>0</sup>b<sup>2</sup>} &rArr; z<sub>2</sub> = 6 = T<sub>3</sub>\n",
    "\n",
    "P<sub>3</sub> &rArr; P<sub>2</sub> &Union; {a<sup>3</sup>b<sup>0</sup>, a<sup>2</sup>b<sup>1</sup>, a<sup>1</sup>b<sup>2</sup>, a<sup>0</sup>b<sup>3</sup>} &rArr; z<sub>2</sub> + 4 = 10 = T<sub>4</sub>\n",
    "\n",
    "Thus, z<sub>n</sub> = T<sub>n+1</sub> and T<sub>11</sub> = z<sub>10</sub> = 55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "\n",
    "Give average hypothesis g-bar resulting from training the same model &Hscr; on different data sets &Dscr; to get g<sup>(&Dscr;)</sup> &isin; &Hscr; and taking the expected value of g<sup>(&Dscr;)</sup> w.r.t. &Dscr;. We want to consider a model &Hscr; that could result in g-bar &notin; &Hscr;.\n",
    "\n",
    "As the logistic regression function &theta;(s) = e<sup>s</sup>/(1+e<sup>s</sup>) is non-linear, we can expect the averaging done by taking the expected value not to be in &Hscr;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "We **cannot** always determine if there is overfitting by comparing the values of (E<sub>out</sub> - E<sub>in</sub>). One major issue is that we have **no** idea of what E<sub>in</sub> is doing. (E<sub>out</sub> - E<sub>in</sub>) = 0.9 - 0.5 = 0.5 -0.1 = 0.4. In the first case, it's arguable that our hypothesis doesn't really match the target function all that well, it's just bad yet our difference vaue is the same as the second case, where our hypothesis is doing relatively well and more of a candidate of overfitting.\n",
    "\n",
    "Deterministic noise is dependent on the hypothesis set since it's the details of the target function f that it cannot possibly capture. Stochastic noise however is not dependent on the hypthesis set, it's just regular old noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Given :** The regularized weight w<sub>reg</sub> solves:\n",
    "\n",
    "minimize (1/N)&sum;(n=1;N){w<sup>T</sup>x<sub>n</sub>-y<sub>n</sub>)<sup>2</sup> subject to w<sup>T</sup>&Gamma;<sup>T</sup>&Gamma;w &le; C\n",
    "\n",
    "where &Gamma; is a  matrix. \n",
    "\n",
    "**Want :** w<sup>T</sup><sub>lin</sub>&Gamma;<sup>T</sup>&Gamma;w<sub>lin</sub> &rArr; w<sub>reg</sub> = ?\n",
    "\n",
    "Since we wish to find the w (w<sub>reg</sub>) that minimizes the given expression and w<sub>lin</sub> already satisfies the given constrants, w<sub>reg</sub> = w<sub>lin</sub>.\n",
    "\n",
    "Furthermore, minimizing the given function results with the given constraints results in minimizing the in-sample error function with an additional slack term, which can be considered an equation for **augmented error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "Using the dataset of HW8 (US Postal Service Zip Code data set) and implementing the same classifiers as before (one-vs-one with one number being +1, the other being -1, rest ignored, and one-vs-all with one number being +1, the rest -1), we wish to use regularized linear regression this time, **minimizing** the following expression:\n",
    "\n",
    "\n",
    "(1/N) &sum;(n=1;N){w<sup>T</sup>z<sub>n</sub>-y<sub>n</sub>)<sup>2</sup> + (&lambda;/N)w<sup>T</sup>w\n",
    "\n",
    "On x-vs-all classifiers, we let &lambda; = 1 and both omit (z = x = (1,x<sub>1</sub>,x<sub>2</sub>)) and apply a feature transform (z = (1,x<sub>1</sub>,x<sub>2</sub>,x<sub>1</sub>x<sub>2</sub>,x<sub>1</sub><sup>2</sup>,x<sub>2</sub><sup>2</sup>))\n",
    "\n",
    "Then we want to use a 1-vs-5 classifier with the feature tranform and compare the performance with &lambda; = 0.01 and &lambda; = 1.\n",
    "\n",
    "\n",
    "**Note** that minimizing the given function above is equivalent to solving for w<sub>reg</sub> = (Z<sup>T</sup>Z + &lambda;I)<sup>-1</sup>Z<sup>T</sup>y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data loader\n",
    "hw8_train = \"hw8_train.dta\"\n",
    "hw8_test = \"hw8_test.dta\"\n",
    "hw8_data = LFD_Data2(hw8_train, hw8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init reglinreg\n",
    "\n",
    "from reglinreg import RegLinReg\n",
    "\n",
    "#argts are l_reg, nlt\n",
    "rlr = RegLinReg(1, False) #no feature transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ 0-vs-all ~~\n",
      "no transform: e_in = 0.109313, e_out = 0.115097\n",
      "transform: e_in = 0.102318, e_out = 0.106627\n",
      "\n",
      "~~ 1-vs-all ~~\n",
      "no transform: e_in = 0.015224, e_out = 0.022422\n",
      "transform: e_in = 0.012344, e_out = 0.021923\n",
      "\n",
      "~~ 2-vs-all ~~\n",
      "no transform: e_in = 0.100261, e_out = 0.098655\n",
      "transform: e_in = 0.100261, e_out = 0.098655\n",
      "\n",
      "~~ 3-vs-all ~~\n",
      "no transform: e_in = 0.090248, e_out = 0.082711\n",
      "transform: e_in = 0.090248, e_out = 0.082711\n",
      "\n",
      "~~ 4-vs-all ~~\n",
      "no transform: e_in = 0.089425, e_out = 0.099651\n",
      "transform: e_in = 0.089425, e_out = 0.099651\n",
      "\n",
      "~~ 5-vs-all ~~\n",
      "no transform: e_in = 0.076258, e_out = 0.079721\n",
      "transform: e_in = 0.076258, e_out = 0.079223\n",
      "\n",
      "~~ 6-vs-all ~~\n",
      "no transform: e_in = 0.091071, e_out = 0.084704\n",
      "transform: e_in = 0.091071, e_out = 0.084704\n",
      "\n",
      "~~ 7-vs-all ~~\n",
      "no transform: e_in = 0.088465, e_out = 0.073244\n",
      "transform: e_in = 0.088465, e_out = 0.073244\n",
      "\n",
      "~~ 8-vs-all ~~\n",
      "no transform: e_in = 0.074338, e_out = 0.082711\n",
      "transform: e_in = 0.074338, e_out = 0.082711\n",
      "\n",
      "~~ 9-vs-all ~~\n",
      "no transform: e_in = 0.088328, e_out = 0.088191\n",
      "transform: e_in = 0.088328, e_out = 0.088191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cur_num in range(10):\n",
    "    hw8_data.set_filter([cur_num]) #setting to x-vs-all\n",
    "    #some data loading\n",
    "    rlr_xtrain = hw8_data.get_X(\"train\")\n",
    "    rlr_ytrain= hw8_data.get_Y(\"train\")\n",
    "    rlr_xtest = hw8_data.get_X(\"test\")\n",
    "    rlr_ytest= hw8_data.get_Y(\"test\")\n",
    "    rlr.set_nlt(False) #no feature transform\n",
    "    rlr.train_reg(rlr_xtrain, rlr_ytrain)\n",
    "    rlr_ein = rlr.calc_error(rlr_xtrain, rlr_ytrain)\n",
    "    rlr_eout = rlr.calc_error(rlr_xtest, rlr_ytest)\n",
    "    print(\"~~ %d-vs-all ~~\" % cur_num)\n",
    "    print(\"no transform: e_in = %f, e_out = %f\" % (rlr_ein, rlr_eout))\n",
    "    rlr.set_nlt(True) #yes feature transform\n",
    "    rlr.train_reg(rlr_xtrain, rlr_ytrain)\n",
    "    rlr_ein = rlr.calc_error(rlr_xtrain, rlr_ytrain)\n",
    "    rlr_eout = rlr.calc_error(rlr_xtest, rlr_ytest)\n",
    "    print(\"transform: e_in = %f, e_out = %f\" % (rlr_ein, rlr_eout))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the no-transform case, out of (5-9)-vs-all, 8 has the lowest E<sub>in</sub>.\n",
    "\n",
    "For the transformed case, out of (0-4)-vs-all, 1 has the lowest E<sub>out</sub>.'\n",
    "\n",
    "The transform improved the out-of-sample performance of 5-vs-all (7.9721% no-transform vs 7.9223% transformed) but by less than 5% (much less, closer to 0.05%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.010000\n",
      "e_in = 0.004484, e_out = 0.028302\n",
      "\n",
      "lambda = 1.000000\n",
      "e_in = 0.005125, e_out = 0.025943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hw8_data.set_filter([1, 5]) #setting to 1-vs-5\n",
    "#some data loading\n",
    "rlr_xtrain = hw8_data.get_X(\"train\")\n",
    "rlr_ytrain= hw8_data.get_Y(\"train\")\n",
    "rlr_xtest = hw8_data.get_X(\"test\")\n",
    "rlr_ytest= hw8_data.get_Y(\"test\")\n",
    "rlr.set_nlt(True) #feature transform\n",
    "\n",
    "#possible lambda regularizers\n",
    "l_reg = [0.01, 1]\n",
    "\n",
    "for cur_lreg in l_reg:\n",
    "    rlr.set_lambda(cur_lreg)\n",
    "    rlr.train_reg(rlr_xtrain, rlr_ytrain)\n",
    "    rlr_ein = rlr.calc_error(rlr_xtrain, rlr_ytrain)\n",
    "    rlr_eout = rlr.calc_error(rlr_xtest, rlr_ytest)\n",
    "    print(\"lambda = %f\" % cur_lreg)\n",
    "    print(\"e_in = %f, e_out = %f\" % (rlr_ein, rlr_eout))\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since from &lambda; = 0.01 to &lambda; = 1, E<sub>in</sub> increases while E<sub>out</sub> decreases, it is clear that the former case exhibits overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "**Given :** training set generated by target function f: &Xscr; &rarr; {-1,+1} where &Xscr; = &reals;<sup>2</sup>.\n",
    "\n",
    "\n",
    "x<sub>1</sub>(1,0) &rarr; -1\n",
    "\n",
    "x<sub>2</sub>(0,1) &rarr; -1\n",
    "\n",
    "x<sub>3</sub>(0,-1) &rarr; -1\n",
    "\n",
    "x<sub>4</sub>(-1,0) &rarr; +1\n",
    "\n",
    "x<sub>5</sub>(0,2) &rarr; +1\n",
    "\n",
    "x<sub>6</sub>(0,-2) &rarr; +1\n",
    "\n",
    "x<sub>7</sub>(-2,0) &rarr; +1\n",
    "\n",
    "\n",
    "Mapping into two-dimension space &Zscr; with:\n",
    "\n",
    "z<sub>1</sub> = x<sub>2</sub><sup>2</sup> - 2x<sub>1</sub> - 1\n",
    "\n",
    "z<sub>2</sub> = x<sub>1</sub><sup>2</sup> - 2x<sub>2</sub> + 1\n",
    "\n",
    "\n",
    "**Want :** w,b &SuchThat; w<sup>T</sup>z + b = 0 maximizes the margin of the &Zscr; space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed set is: \n",
      "[[-3.  2.]\n",
      " [ 0. -1.]\n",
      " [ 0.  3.]\n",
      " [ 1.  2.]\n",
      " [ 3. -3.]\n",
      " [ 3.  5.]\n",
      " [ 3.  5.]]\n"
     ]
    }
   ],
   "source": [
    "#calculating the X stransform and setting up arrays.\n",
    "\n",
    "fsvm_x = np.array([[1,0],[0,1], [0,-1], [-1,0], [0,2],[0,-2], [-2,0]])\n",
    "fsvm_y = np.array([-1,-1,-1,1,1,1,1])\n",
    "fsvm_z = np.ndarray((0,2))\n",
    "for x in fsvm_x:\n",
    "    z1 = pow(x[1],2.0)-(2.0*x[0])-1.0\n",
    "    z2 = pow(x[0],2.0)-(2.0*x[1])+1.0\n",
    "    z = np.array([z1,z2])\n",
    "    fsvm_z = np.vstack((fsvm_z,z))\n",
    "\n",
    "print(\"The transformed set is: \")\n",
    "print(fsvm_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the outputs of candidate w plugged into the plane eq are:\n",
      "[[ 4.5 -1.5  2.5  0.5 -6.5  1.5  1.5]\n",
      " [-5.5  0.5 -3.5 -1.5  5.5 -2.5 -2.5]\n",
      " [-3.5 -0.5 -0.5  0.5  2.5  2.5  2.5]\n",
      " [ 1.5 -1.5  2.5  1.5 -3.5  4.5  4.5]]\n",
      "\n",
      "margins of candidates w (a,b,c,d):\n",
      "[ -8.5   7.5  12.5   4.5]\n"
     ]
    }
   ],
   "source": [
    "#for the heck of it, let's do this by brute force\n",
    "#and go through all the given options to find the values of w and b\n",
    "\n",
    "#our candidates\n",
    "fsvm_wopt = np.array([[-0.5,-1,1],[-0.5,1,-1],[-0.5,1,0],[-0.5,0,1]])\n",
    "fsvm_zt = np.c_[np.ones(fsvm_z.shape[0]),fsvm_z]\n",
    "\n",
    "fsvm_yt = np.ndarray((0,fsvm_y.shape[0]))\n",
    "\n",
    "for w in fsvm_wopt:\n",
    "    cur_y = np.matmul(fsvm_zt, w.T)\n",
    "    fsvm_yt = np.vstack((fsvm_yt, cur_y))\n",
    "#multiplying by proper labels fsvm_y because we\n",
    "#want the given params to actually AGREE with the output of the separating plane \n",
    "fsvm_ydist = np.sum(np.multiply(fsvm_y, fsvm_yt), axis=1)\n",
    "\n",
    "print(\"\\nthe outputs of candidate w plugged into the plane eq are:\")\n",
    "print(fsvm_yt)\n",
    "print(\"\\nmargins of candidates w (a,b,c,d):\")\n",
    "print(fsvm_ydist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the preceding output, we see that w<sub>1</sub>, w<sub>2</sub>, b = 1,0,-0.5 gives us the maximal margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of support vector for svm with degree 2 polynomial kernel: 5\n"
     ]
    }
   ],
   "source": [
    "#hard margin svm -> C = 0\n",
    "my_svm = SVM_Poly(2,0) #degree 2 polynomial kernel with hard-margin\n",
    "my_svm.train(fsvm_x, fsvm_y)\n",
    "cur_numalphas = my_svm.num_alphas\n",
    "print(\"number of support vector for svm with degree 2 polynomial kernel: %d\" % cur_numalphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a hard-margin SVM with the kernel:\n",
    "\n",
    "K(x,x') = (1+x<sup>T</sup>x')<sup>2</sup>\n",
    "\n",
    "We end up with an SVM with 5 supoort vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Functions\n",
    "\n",
    "Experimenting wtih the RBF model **a)** in regular form (Lloyd's algorithm + psuedo-inverse) with K centers:\n",
    "\n",
    "\n",
    "sign(&sum; (k=1;K){w<sub>k</sub> exp(-&gamma; &Vert;x-&mu;<sub>k</sub>&Vert;<sup>2</sup>) + b)\n",
    "\n",
    "\n",
    "and using w = (&Phi;<sup>T</sup>&Phi;)<sup>-1</sup>&Phi;<sup>T</sup>y, where &Phi; is the matrix made up the given expression above (or basically we want the psuedo-inverse of &Phi; times y)\n",
    "\n",
    "\n",
    "and in **b)** kernel form using a hard-margin SVM:\n",
    "\n",
    "\n",
    "sign(&sum; (&alpha;<sub>n</sub> &gt; 0 ){&alpha;<sub>n</sub>y<sub>n</sub> exp(-&gamma; &Vert;x-x;<sub>n</sub>&Vert;<sup>2</sup>) + b)\n",
    "\n",
    "**Given :** target func f(x) = sign(x<sub>2</sub>-x<sub>1</sub> + 0.25(sin(&pi;x<sub>1</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class for generating dataset\n",
    "\n",
    "class RbfFData:\n",
    "    def __init__(self, n):\n",
    "        #n = number of data points\n",
    "        n = max(1, n)\n",
    "        X = np.random.uniform(-1,1, (n, 2))\n",
    "        X_sin = np.sin(np.multiply(np.pi, X[:,0]))\n",
    "        X_oper = np.array([-1, 1, 0.25])\n",
    "        X_res = np.c_[X, X_sin]\n",
    "        self.X = X\n",
    "        self.Y = np.sign(np.matmul(X_res, X_oper))\n",
    "        \n",
    "#importing my SVM_RBF kernel class\n",
    "from svm_kernel import SVM_RBF\n",
    "from rbf import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 1.5 | SVM non-separable pct: 0.000000%\n",
      "gamma = 1.5, k = 9 | SVM beats reg RBF pct: 85.500000%\n",
      "gamma = 1.5, k = 12 | SVM beats reg RBF pct: 78.500000%\n",
      "gamma = 1.5, k = 9 | reg RBF E_in = 0 pct: 0.000000%\n",
      "\n",
      "~~gamma = 1.5 k=9~~\n",
      "E_in: 4.975000%\n",
      "E_out: 6.825000%\n",
      "\n",
      "~~gamma = 1.5 k=12~~\n",
      "E_in: 3.630000%\n",
      "E_out: 6.080000%\n",
      "\n",
      "~~gamma = 2 k=9~~\n",
      "E_in: 5.370000%\n",
      "E_out: 7.200000%\n"
     ]
    }
   ],
   "source": [
    "n_train = 100\n",
    "n_runs = 200\n",
    "gamma = [1.5, 2]\n",
    "thresh = 1.0e-100\n",
    "my_svm = SVM_RBF(gamma[0])\n",
    "rbf_k = [9, 12]\n",
    "rbf_rng = [-1,1]\n",
    "\n",
    "#gamma, k\n",
    "ein_15_9 = np.array([])\n",
    "ein_15_12 = np.array([])\n",
    "ein_2_9 = np.array([])\n",
    "eout_15_9 = np.array([])\n",
    "eout_15_12 = np.array([])\n",
    "eout_2_9 = np.array([])\n",
    "\n",
    "#kernel beats regular in eout\n",
    "svm_beats_rbf9 = np.array([])\n",
    "svm_beats_rbf12 = np.array([])\n",
    "\n",
    "#svm nonseparable (ein not 0)\n",
    "svm_nonsep = np.array([])\n",
    "\n",
    "#ein is 0 for rbf, gamma 1.5, k = 9\n",
    "rbf9_ein0 = np.array([])\n",
    "\n",
    "for run in range(n_runs):\n",
    "    cur_train = RbfFData(n_train)\n",
    "    cur_test = RbfFData(n_train)\n",
    "    my_rbf = RBF(gamma[0], cur_train.X, cur_train.Y, rbf_k[0], rbf_rng)\n",
    "    my_svm.train(cur_train.X, cur_train.Y)\n",
    "    svm_ein = my_svm.calc_error(cur_train.X, cur_train.Y)\n",
    "    svm_eout = my_svm.calc_error(cur_test.X, cur_test.Y)\n",
    "    svm_nonsep = np.concatenate((svm_nonsep, [svm_ein > thresh]))\n",
    "    for k in rbf_k:\n",
    "        for g in gamma:\n",
    "            my_rbf.set_k(k)\n",
    "            my_rbf.set_gamma(g)\n",
    "            my_rbf.train()\n",
    "            if k == 9 and g == 1.5:\n",
    "                cein = my_rbf.calc_error(cur_train.X, cur_train.Y)\n",
    "                cout = my_rbf.calc_error(cur_test.X, cur_test.Y)\n",
    "                ein_15_9 = np.concatenate((ein_15_9, [cein]))\n",
    "                eout_15_9 = np.concatenate((eout_15_9, [cout]))\n",
    "                svm_beats_rbf9 = np.concatenate((svm_beats_rbf9, [svm_eout < cout]))\n",
    "            elif k == 12 and g == 1.5:\n",
    "                cein = my_rbf.calc_error(cur_train.X, cur_train.Y)\n",
    "                cout = my_rbf.calc_error(cur_test.X, cur_test.Y)\n",
    "                ein_15_12 = np.concatenate((ein_15_12, [cein]))\n",
    "                eout_15_12 = np.concatenate((eout_15_12, [cout]))\n",
    "                svm_beats_rbf12 = np.concatenate((svm_beats_rbf12, [svm_eout < cout]))\n",
    "            elif k == 9 and g == 2:\n",
    "                cein = my_rbf.calc_error(cur_train.X, cur_train.Y)\n",
    "                cout = my_rbf.calc_error(cur_test.X, cur_test.Y)\n",
    "                ein_2_9 = np.concatenate((ein_2_9, [cein]))\n",
    "                eout_2_9 = np.concatenate((eout_2_9, [cout]))\n",
    "            \n",
    "pct_svm_nonsep =  100.0 * np.sum(svm_nonsep)/float(n_runs)\n",
    "pct_rbf9_loses = 100.0 * np.sum(svm_beats_rbf9)/float(n_runs)\n",
    "pct_rbf12_loses = 100.0 * np.sum(svm_beats_rbf12)/float(n_runs)\n",
    "pct_rbf9_ein0 = 100.0 * np.sum(rbf9_ein0)/float(n_runs)\n",
    "avg_ein_15_9 = 100.0 * np.average(ein_15_9)\n",
    "avg_eout_15_9 = 100.0 * np.average(eout_15_9)\n",
    "avg_ein_15_12 = 100.0 * np.average(ein_15_12)\n",
    "avg_eout_15_12 = 100.0 * np.average(eout_15_12)\n",
    "avg_ein_2_9 = 100.0 * np.average(ein_2_9)\n",
    "avg_eout_2_9 = 100.0 *np.average(eout_2_9)\n",
    "\n",
    "print(\"gamma = 1.5 | SVM non-separable pct: %f%%\" % pct_svm_nonsep)\n",
    "print(\"gamma = 1.5, k = 9 | SVM beats reg RBF pct: %f%%\" % pct_rbf9_loses)\n",
    "print(\"gamma = 1.5, k = 12 | SVM beats reg RBF pct: %f%%\" % pct_rbf12_loses)\n",
    "print(\"gamma = 1.5, k = 9 | reg RBF E_in = 0 pct: %f%%\" % pct_rbf9_ein0)\n",
    "print(\"\")\n",
    "print(\"~~gamma = 1.5 k=9~~\")\n",
    "print(\"E_in: %f%%\" % avg_ein_15_9)\n",
    "print(\"E_out: %f%%\" % avg_eout_15_9)\n",
    "print(\"\")\n",
    "print(\"~~gamma = 1.5 k=12~~\")\n",
    "print(\"E_in: %f%%\" % avg_ein_15_12)\n",
    "print(\"E_out: %f%%\" % avg_eout_15_12)\n",
    "print(\"\")\n",
    "print(\"~~gamma = 2 k=9~~\")\n",
    "print(\"E_in: %f%%\" % avg_ein_2_9)\n",
    "print(\"E_out: %f%%\" % avg_eout_2_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that as k increases, both E<sub>in</sub> and E<sub>out</sub> drop and when &gamma; increases, both E<sub>in</sub> and E<sub>out</sub> increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
