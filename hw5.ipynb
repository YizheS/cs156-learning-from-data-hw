{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# HW 5\n",
    "## Linear Regression Error\n",
    "\n",
    "Given noisy target y = **w**<sup>*T</sup>x + &epsilon; &SuchThat; x &isin; &reals;<sup>d</sup> (with x<sub>0</sub> = 1), y &isin; &reals;, w* unknown vector, &epsilon; noise term with zero mean and &sigma;<sup>2</sup> variance; data set &Dscr; = {(x1, y1),...,(xN, yN)} and the equation:\n",
    "\n",
    "\n",
    "&#120124;<sub>&#119967;</sub>[E<sub>in</sub>(w<sub>lin</sub>)] = &sigma;<sup>2</sup>(1 - (d+1)/N)\n",
    "\n",
    "\n",
    "Let the LH be &tau;, then:\n",
    "\n",
    "&tau;/(&sigma;<sup>2</sup>) = 1 - (d+1)/N\n",
    "\n",
    "&rArr; (d+1)/N = 1-&tau;/(&sigma;<sup>2</sup>)\n",
    "\n",
    "&rArr; (d+1)/(1-&tau;/(&sigma;<sup>2</sup>)) = N\n",
    "\n",
    "\n",
    "\n",
    "For &sigma; = 0.1, d = 8, E<sub>in</sub> &gt; 0.008, want smallest N.\n",
    "\n",
    "&rArr; 9/(1 - 0.008/(0.1<sup>2</sup>)) < N\n",
    "\n",
    "&rArr; 45 < N\n",
    "\n",
    "\n",
    "Thus the smallest given N that satisfies this is **100**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Nonlinear Transforms\n",
    "\n",
    "**Given :** &phi;(1, x1, x2) = (1, x1<sup>2</sup>, x2<sup>2</sup>)\n",
    "\n",
    "**Want :** weights giving a hyperbolic decision boundary\n",
    "\n",
    "\n",
    "\n",
    "Since the general equation of a hyperbola is x<sup>2</sup>/a<sup>2</sup> - y<sup>2</sup>/b<sup>2</sup> = 1 and we want h(x) to be negative when x1<sup>2</sup> is large, the correct weights should be **w1 < 0, w2 > 0**.\n",
    "\n",
    "**Given :** &phi;<sub>4</sub>: x &rarr; (1, x1, x1<sup>2</sup>, x1*x2, x2<sup>2</sup>, x1<sup>3</sup>, x1<sup>2</sup>*x2, x1*x2<sup>2</sup>, x2<sup>3</sup>, x1<sup>4</sup>, x1<sup>3</sup>*x2, x1<sup>2</sup>*x2<sup>2</sup>, x1*x2<sup>3</sup>, x2<sup>4</sup>), d = 13.\n",
    "\n",
    "**Want :** VC dimension\n",
    "\n",
    "\n",
    "Since VC dimension for a linear model = d + 1, d<sub>VC</sub> = 14, thus the lowest number given not smaller than the VC dimension is **15**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "**Given :** nonlinear error surface E(u,v) = (u*e<sup>v</sup> - 2*v*e<sup>-u</sup>)<sup>2</sup>, starting point (u,v) = (1,1), learning rate &eta; = 0.1\n",
    "\n",
    "- &part;E/&part;u = 2*(e<sup>v</sup> + 2*v*e<sup>-u</sup>) * (u*e<sup>v</sup> - 2*v*e<sup>-u</sup>)\n",
    "- &part;E/&part;v = 2*(u*e<sup>v</sup> - 2*e<sup>-u</sup>)*(u*e<sup>v</sup> - 2*v*e<sup>-u</sup>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With starting coordinates (1.000000,1.000000), learning rate 0.100000 and threshold 1.000000e-13:\n",
      "It took 10 iterations to achieve an error of 1.208683e-15 ending at coordinates (0.044736,0.023959).\n",
      "Iterating coordinate-wise for 15 iterations, the resulting error is 0.139814.\n"
     ]
    }
   ],
   "source": [
    "gd_thresh = 10e-14\n",
    "gd_lrate = 0.1\n",
    "gd_start = np.array([1,1])\n",
    "gd_coord_iters = 15\n",
    "def gd_error(cur_coords):\n",
    "    u = cur_coords[0]\n",
    "    v = cur_coords[1]\n",
    "    cur_error = math.pow(u*math.pow(math.e, v) - 2.0*v*math.pow(math.e, -1 * u), 2.0)\n",
    "    return cur_error\n",
    "\n",
    "def gd_partial(cur_coords):\n",
    "    u = cur_coords[0]\n",
    "    v = cur_coords[1]\n",
    "    u_coord = 2.0*(math.pow(math.e, v) + 2.0*v*math.pow(math.e, -1 * u)) * (u*math.pow(math.e,v) - 2.0*v*math.pow(math.e, -1 * u))\n",
    "    v_coord = 2.0*(u*math.pow(math.e, v) - 2.0*math.pow(math.e, -1 * u))*(u*math.pow(math.e, v) - 2*v*math.pow(math.e, -1 * u))\n",
    "    return np.array([u_coord, v_coord])\n",
    "\n",
    "def gd_perform(coords, l_rate, threshold):\n",
    "    num_iterations = 0\n",
    "    cur_error = gd_error(coords)\n",
    "    while cur_error >= threshold:\n",
    "        num_iterations = num_iterations + 1\n",
    "        cur_partial = gd_partial(coords)\n",
    "        coords = np.subtract(coords, np.multiply(l_rate, cur_partial))\n",
    "        cur_error = gd_error(coords)\n",
    "    return coords, cur_error, num_iterations\n",
    "\n",
    "def gd_coord_perform(coords, l_rate, iters):\n",
    "    #instead of moving along both coords, moving along u, then v, then u, then v...\n",
    "    cur_error = 0\n",
    "    for cur_iter in range(iters):\n",
    "        cur_partial = gd_partial(coords)\n",
    "        #first move along u coord so 0 out v coord\n",
    "        coords = np.subtract(coords, np.multiply(l_rate, np.multiply(np.array([1,0]), cur_partial)))\n",
    "        #now redo for v coord\n",
    "        cur_partial = gd_partial(coords)\n",
    "        coords = np.subtract(coords, np.multiply(l_rate, np.multiply(np.array([0,1]), cur_partial)))\n",
    "        cur_error = gd_error(coords)\n",
    "    return cur_error\n",
    "\n",
    "\n",
    "gd_coords, gd_err, gd_numiters = gd_perform(gd_start, gd_lrate, gd_thresh)\n",
    "gd_cerr = gd_coord_perform(gd_start, gd_lrate, gd_coord_iters)\n",
    "print(\"With starting coordinates (%f,%f), learning rate %f and threshold %e:\" % (gd_start[0], gd_start[1], gd_lrate, gd_thresh))\n",
    "print(\"It took %d iterations to achieve an error of %e ending at coordinates (%f,%f).\" % (gd_numiters, gd_err, gd_coords[0], gd_coords[1]))\n",
    "print(\"Iterating coordinate-wise for %d iterations, the resulting error is %f.\" % (gd_coord_iters, gd_cerr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "**Want :** Logistic regression on user-generated target function f with data set D where f indicates a 0/1 probability. Generate N = 100 points on &Xscr; = [-1,1] &Cross; [-1,1] and let f be defined by the line passing through two additional points on [-1,1] &Cross; [-1,1] and serving as the boundary between y = &PlusMinus;1. Initialize the weight vector w to all zeros and generate another large set of points for E<sub>out</sub>. Stop the algorithm when &Vert; w<sup>(t-1)</sup> - w<sup>(t)</sup> &Vert; &lt; 0.01.\n",
    "\n",
    "\n",
    "**Note :** My code for logistic regression is in logreg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To converge on N=100 training examples:\n",
      "it took logistic regression an average 325.220000 epochs with an average E_out of 0.096746\n"
     ]
    }
   ],
   "source": [
    "from logreg import LogReg\n",
    "\n",
    "LR_EXP = 100 #number of times to run experiment\n",
    "LR_N = 100 #number of points for training set, use for E_out also\n",
    "LR_WTHRESH = 0.01 #desired weight change threshold\n",
    "\n",
    "class Line:\n",
    "    def __init__(self, p1, p2):\n",
    "        #input: 2 2-dim numpy arrays\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        diff = np.subtract(p2, p1)\n",
    "        if diff[0] <= 0.001:\n",
    "            #if vertical (or close to it), just set slope to none\n",
    "            self.slope = None\n",
    "            self.is_vert = True\n",
    "        else:\n",
    "            self.slope = diff[1]/diff[0]\n",
    "            self.is_vert = False\n",
    "        #point slope form = y - y1 = m(x - x1) \n",
    "        #y = 0 -> -y1 = m(x - x1) -> -y1/m = x - x1 -> (-y1/m) + x1 = x\n",
    "        if not self.is_vert:\n",
    "            self.y_int = ((-1 * p1[1])/self.slope) + p1[0]\n",
    "\n",
    "        \n",
    "    def calc(self,testpt):\n",
    "        #input: numpy array with 2 dim\n",
    "        #goal: test against equation of line, if above, then return +1\n",
    "        #if on line 0, else -1\n",
    "        #to check:\n",
    "        #if vertical, check against x, else check against y\n",
    "\n",
    "        #slope-intercept: y = mx + b or (y-b)/m = x\n",
    "        if self.is_vert == False:\n",
    "            line_y = self.slope*testpt[0] + self.y_int\n",
    "            diff = testpt[1] - line_y\n",
    "        else:\n",
    "            line_x = self.p1[0]\n",
    "            diff = testpt[0] - line_x\n",
    "        return np.sign(diff)\n",
    "    \n",
    "    def calc_pts(self, ptset):\n",
    "        #batch calculate points\n",
    "        pt_dim = ptset.shape[1]\n",
    "        my_calc = np.array([])\n",
    "        for pt in ptset:\n",
    "            cur_calc = self.calc(pt)\n",
    "            my_calc = np.concatenate((my_calc, [cur_calc]))\n",
    "        return my_calc\n",
    "        \n",
    "\n",
    "cur_logreg = LogReg(2, 0.01) #new logreg class with dim = 2 and learning rate 0.01\n",
    "lr_epochs = np.array([]) #epoch record keeping\n",
    "lr_eout = np.array([]) #e_out record keeping\n",
    "for i in range(LR_EXP):\n",
    "    #print(i)\n",
    "    cur_logreg.init_weights() #reset weights to 0\n",
    "    cur_lpts = np.random.uniform(-1, 1, (2,2)) #generate points for line\n",
    "    cur_line = Line(cur_lpts[0], cur_lpts[1])\n",
    "    cur_train = np.random.uniform(-1,1,(LR_N,2)) #generate training set\n",
    "    cur_labels = cur_line.calc_pts(cur_train) #get labels\n",
    "    cur_epochs = 0 #init number of epochs\n",
    "    cur_wdiff = 100 #init weight difference\n",
    "    while cur_wdiff >= LR_WTHRESH:\n",
    "        cur_epochs = cur_epochs + 1\n",
    "        w_t = cur_logreg.weights #weights before training\n",
    "        cur_logreg.sto_gd(cur_train, cur_labels) #run stochastic gradient descent which randomizes order of entries\n",
    "        w_tp1 = cur_logreg.weights #weights after training\n",
    "        cur_wdiff = np.linalg.norm(np.subtract(w_tp1, w_t)) #condition\n",
    "    lr_epochs = np.concatenate((lr_epochs, [cur_epochs])) \n",
    "    cur_oos = np.random.uniform(-1,1, (LR_N, 2)) #out of sample testing\n",
    "    oos_labels = cur_line.calc_pts(cur_oos)\n",
    "    cur_eout = cur_logreg.ce_error(cur_oos, oos_labels)\n",
    "    lr_eout = np.concatenate((lr_eout,[cur_eout]))\n",
    "\n",
    "lr_epochs_avg = np.average(lr_epochs)\n",
    "lr_eout_avg = np.average(lr_eout)\n",
    "\n",
    "\n",
    "print(\"To converge on N=%d training examples:\" % LR_N)\n",
    "print(\"it took logistic regression an average %f epochs with an average E_out of %f\" % (lr_epochs_avg, lr_eout_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA as SGD\n",
    "\n",
    "\n",
    "The PLA learning algo as it is uses the equation w(t+1) = w(t) + yx where x is a misclassified point. That is, when sign(w<sup>T</sup>x<sub>n</sub>) &ne; y<sub>n</sub>. Let  &kappa; = y<sub>n</sub>w<sup>T</sup>x<sub>n</sub>. Then in this case, &kappa; &le; 0 because of the mismatched signs between the two quantities. Since we only want to update the weight with **mismatched** points and stochastic gradient descent takes into account all points, we need to take min(0, &kappa;) and since in SGD we want to **minimize** an error function and all values (besides 0) that will come out of that minimum are negative, we negate the quantity thus a proper error function for PLA in SGD is:\n",
    "\n",
    "\n",
    "-min(0, y<sub>n</sub>w<sup>T</sup>x<sub>n</sub>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "hw5.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
